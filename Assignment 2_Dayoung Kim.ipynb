{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dayoung (Leanna) Kim\n",
    "\n",
    "Natural Language Processing &Health \n",
    "Assignment 2 \n",
    "\n",
    "1. (30 pts) Given the training data  \n",
    "\\<s> John read her book \\</s> \n",
    "\n",
    "\\<s> I read a different book \\</s> \n",
    "\n",
    "\\<s> John read a book by Mulan \\</s> \n",
    "\n",
    "a. (10 pts) Manually list and calculate bigrams using maximum likelihood estimates  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b> Table for 1-a and 2-a </b></center>\n",
    "\n",
    "Bigrams |MLE counts|Add-1 counts\n",
    "--------|----------|------------\n",
    "\\<s>,John|2|3\n",
    "John,\\<s>|0|1\n",
    "John,read|2|3\n",
    "read,John|0|1\n",
    "read,her|1|2\n",
    "her,read|0|1\n",
    "her,book|1|2\n",
    "book,her|0|1\n",
    "book,\\</s>|2|3\n",
    "\\</s>,book|0|1\n",
    "\\</s>,I|0|1\n",
    "I,\\</s>|0|1\n",
    "I,a|0|1\n",
    "a,I|0|1\n",
    "a,different|1|2\n",
    "different,a|0|1\n",
    "different,by|0|1\n",
    "by,different|0|1\n",
    "by,Mulan|1|2\n",
    "Mulan,by|0|1\n",
    "Mulan,\\<UNK>|0|1\n",
    "\\<UNK>,Mulan|0|1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Table for 1-b and 2-b\n",
    "    <br>MLE P(w(i)|w(i-1)) = counts(w(i-1),w(i))/counts(w(i-1))\n",
    "    <br>Add-1 P(w(i)|w(i-1)) = (counts(w(i-1),w(i))+1)/(counts(w(i-1))+v), (v=11, vocab size)\n",
    "    <br> (Each value represents P(row|column))</b></center>\n",
    "    \n",
    "MLE Prob  |  \\<s> |   John  |  read  |  her |   book  | \\</s>  |  I | a  |  different |   by  |  Mulan |   \\<UNK>\n",
    "--------- | -----  |------ | ------ | ----- | ------  |------ | ---  |---  |----------- | ---- | ------- | -------\n",
    "\\<s>       |0       |    0  | 0      |    0  | 0       |    0  |  0  |0     |         0  |   0  |      0  |      0\n",
    "John      | 0.667  |     0 |  0     |     0 |  0      |     0 |   0 | 0    |          0 |    0 |       0 |       0\n",
    "read      | 0      |     1 |  0     |     0  | 0      |     0 |   1 | 0    |       0    | 0    |    0    |    0\n",
    "her       | 0      |     0 |  0.333 |     0  | 0      |     0 |   0 | 0    |          0 |    0 |       0 |       0\n",
    "book      | 0      |     0 |  0     |     1  | 0       |    0 |   0 | 0.5  |          1 |    0 |       0 |       0\n",
    "\\</s>     | 0      |     0 |  0     |     0  | 0.667   |    0 |   0 | 0    |          0 |    0 |       1 |       0\n",
    "I         | 0.333  |     0 |  0     |     0  | 0       |    0 |   0 | 0   |           0 |    0  |      0  |      0\n",
    "a         | 0      |     0 |  0.667 |     0  | 0       |    0  |  0 | 0    |          0 |    0   |     0  |      0\n",
    "different | 0      |     0 |  0     |     0  | 0       |    0  |  0 | 0.5  |          0 |    0   |     0  |      0\n",
    "by        | 0      |     0 |  0     |     0  | 0.333   |    0  |  0 | 0    |          0 |    0   |     0  |      0\n",
    "Mulan     | 0      |     0 |  0      |    0  | 0       |    0  |  0 | 0    |          0 |    1   |     0  |      0\n",
    "\\<UNK>    | 0      |     0 |  0      |    0  | 0       |    0  |  0 | 0    |          0 |    0   |     0  |      0\n",
    "    \n",
    "Add-1 Prob | \\<s> | John   | read   | her   | book   | \\</s>  | I  |a  |  different  | by | Mulan|  \\<UNK>\n",
    "----- | -----  |------ | ------ | ----- | ------ | ------ | ----- | ----- | --------- | ----- | ------- | ------\n",
    "\\<s>     |  0.071  | 0.077 |  0.071 | 0.083 |  0.071 |  0.071 | 0.083 | 0.077  | 0.083  |0.083   | 0.083  |  0.091\n",
    "John     |  0.214  | 0.077 |  0.071 | 0.083 |  0.071 |  0.071 | 0.083 | 0.077  |  0.083 | 0.083  |  0.083 | 0.091\n",
    "read     |  0.071  | 0.231 |  0.071 | 0.083 |  0.071  | 0.071 | 0.167 | 0.077  |  0.083 | 0.083  |  0.083 | 0.091\n",
    "her      |  0.071  | 0.077 |  0.143 | 0.083 |  0.071  | 0.071 | 0.083 | 0.077  | 0.083|  0.083|  0.083 |0.091\n",
    "book     |  0.071  | 0.077 |  0.071 | 0.167  | 0.071  | 0.071 | 0.083 | 0.154  | 0.167|  0.083  |  0.083 |0.091\n",
    "\\</s>     |  0.071 |  0.077 |  0.071 | 0.083 |  0.214 |  0.071|0.083 | 0.077   |0.083 | 0.083 | 0.167 |0.091\n",
    "I        |  0.143  | 0.077  | 0.071 | 0.083  | 0.071  | 0.071 | 0.083|  0.077  | 0.083|  0.083   | 0.083 |0.091\n",
    "a        |  0.071  | 0.077  | 0.214 | 0.083  | 0.071  | 0.071 | 0.083|  0.077  |0.083 | 0.083   | 0.083  | 0.091\n",
    "different|  0.071  | 0.077 |  0.071 | 0.083  | 0.071  | 0.071 | 0.083|  0.154   |0.083 | 0.083 |   0.083 |  0.091\n",
    "by        | 0.071  | 0.077 |  0.071 | 0.083  | 0.143  | 0.071 | 0.083|  0.077 |0.083 | 0.083  |  0.083 | 0.091\n",
    "Mulan     | 0.071  | 0.077 |  0.071 | 0.083  | 0.071  | 0.071 | 0.083|  0.077 |0.083 | 0.167  |  0.083 | 0.091\n",
    "\\<UNK>     | 0.071  | 0.077 |  0.071 | 0.083 |  0.071 |  0.071|  0.083|  0.077| 0.083|  0.083 |   0.083 | 0.091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk==3.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: click in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from nltk==3.5) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from nltk==3.5) (4.54.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from nltk==3.5) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: regex in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from nltk==3.5) (2020.11.13)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dykim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['John read her book', 'I read a different book', 'John read a book by Mulan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['John', 'read', 'her', 'book'],\n",
       " ['I', 'read', 'a', 'different', 'book'],\n",
       " ['John', 'read', 'a', 'book', 'by', 'Mulan']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = []\n",
    "for sent in text:\n",
    "  tokenized_text.append(word_tokenize(sent))\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'John', 'read', 'her', 'book', '</s>'],\n",
       " ['<s>', 'I', 'read', 'a', 'different', 'book', '</s>'],\n",
       " ['<s>', 'John', 'read', 'a', 'book', 'by', 'Mulan', '</s>']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "padded_tokenized_text = []\n",
    "for sent in tokenized_text:\n",
    "  l = list(pad_both_ends(sent, n=2))\n",
    "  padded_tokenized_text.append(l)\n",
    "padded_tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'John'), ('John', 'read'), ('read', 'her'), ('her', 'book'), ('book', '</s>')]\n",
      "[('<s>', 'I'), ('I', 'read'), ('read', 'a'), ('a', 'different'), ('different', 'book'), ('book', '</s>')]\n",
      "[('<s>', 'John'), ('John', 'read'), ('read', 'a'), ('a', 'book'), ('book', 'by'), ('by', 'Mulan'), ('Mulan', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "for sent in padded_tokenized_text:\n",
    "    print(list(bigrams(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "training_ngrams, padded_sentences = padded_everygram_pipeline(2, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('John',), ('read',), ('her',), ('book',), ('</s>',), ('<s>', 'John'), ('John', 'read'), ('read', 'her'), ('her', 'book'), ('book', '</s>')]\n",
      "[('<s>',), ('I',), ('read',), ('a',), ('different',), ('book',), ('</s>',), ('<s>', 'I'), ('I', 'read'), ('read', 'a'), ('a', 'different'), ('different', 'book'), ('book', '</s>')]\n",
      "[('<s>',), ('John',), ('read',), ('a',), ('book',), ('by',), ('Mulan',), ('</s>',), ('<s>', 'John'), ('John', 'read'), ('read', 'a'), ('a', 'book'), ('book', 'by'), ('by', 'Mulan'), ('Mulan', '</s>')]\n",
      "#############\n",
      "['<s>', 'John', 'read', 'her', 'book', '</s>', '<s>', 'I', 'read', 'a', 'different', 'book', '</s>', '<s>', 'John', 'read', 'a', 'book', 'by', 'Mulan', '</s>']\n"
     ]
    }
   ],
   "source": [
    "training_ngrams = [list(ngramlize_sent) for ngramlize_sent in training_ngrams]\n",
    "for ngramlize_sent in training_ngrams:\n",
    "    print(list(ngramlize_sent))\n",
    "print('#############')\n",
    "padded_sentences = list(padded_sentences)\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 12 items>\n"
     ]
    }
   ],
   "source": [
    "model.fit(training_ngrams, padded_sentences)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 39 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for v in model.vocab:\n",
    "    vocab.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count([<s>,John]）: 2\n",
      "count([John,<s>]）: 0\n",
      "count([John,read]）: 2\n",
      "count([read,John]）: 0\n",
      "count([read,her]）: 1\n",
      "count([her,read]）: 0\n",
      "count([her,book]）: 1\n",
      "count([book,her]）: 0\n",
      "count([book,</s>]）: 2\n",
      "count([</s>,book]）: 0\n",
      "count([</s>,I]）: 0\n",
      "count([I,</s>]）: 0\n",
      "count([I,a]）: 0\n",
      "count([a,I]）: 0\n",
      "count([a,different]）: 1\n",
      "count([different,a]）: 0\n",
      "count([different,by]）: 0\n",
      "count([by,different]）: 0\n",
      "count([by,Mulan]）: 1\n",
      "count([Mulan,by]）: 0\n",
      "count([Mulan,<UNK>]）: 0\n",
      "count([<UNK>,Mulan]）: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(vocab)-1):\n",
    "    v1 = vocab[i]\n",
    "    v2 = vocab[i+1]\n",
    "    print('count(%s）: %s' % ('['+ v1 + ','+v2+']' , model.counts[[v1]][v2]))\n",
    "    print('count(%s）: %s' % ('['+ v2 + ','+v1+']' , model.counts[[v2]][v1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             <s>    John    read    her    book    </s>    I    a    different    by    Mulan    <UNK>\n",
      "---------  -----  ------  ------  -----  ------  ------  ---  ---  -----------  ----  -------  -------\n",
      "<s>            0       2       0      0       0       0    1    0            0     0        0        0\n",
      "John           0       0       2      0       0       0    0    0            0     0        0        0\n",
      "read           0       0       0      1       0       0    0    2            0     0        0        0\n",
      "her            0       0       0      0       1       0    0    0            0     0        0        0\n",
      "book           0       0       0      0       0       2    0    0            0     1        0        0\n",
      "</s>           0       0       0      0       0       0    0    0            0     0        0        0\n",
      "I              0       0       1      0       0       0    0    0            0     0        0        0\n",
      "a              0       0       0      0       1       0    0    0            1     0        0        0\n",
      "different      0       0       0      0       1       0    0    0            0     0        0        0\n",
      "by             0       0       0      0       0       0    0    0            0     0        1        0\n",
      "Mulan          0       0       0      0       0       1    0    0            0     0        0        0\n",
      "<UNK>          0       0       0      0       0       0    0    0            0     0        0        0\n"
     ]
    }
   ],
   "source": [
    "table = [vocab]\n",
    "for i in range(0,len(vocab)):\n",
    "    temp = [vocab[i]]\n",
    "    for j in range(0,len(vocab)):\n",
    "        temp.append(model.counts[[vocab[i]]][vocab[j]])\n",
    "    table.append(temp)\n",
    "print(tabulate(table, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             <s>    John    read    her    book    </s>    I    a    different    by    Mulan    <UNK>\n",
      "---------  -----  ------  ------  -----  ------  ------  ---  ---  -----------  ----  -------  -------\n",
      "<s>        0           0   0          0   0           0    0  0              0     0        0        0\n",
      "John       0.667       0   0          0   0           0    0  0              0     0        0        0\n",
      "read       0           1   0          0   0           0    1  0              0     0        0        0\n",
      "her        0           0   0.333      0   0           0    0  0              0     0        0        0\n",
      "book       0           0   0          1   0           0    0  0.5            1     0        0        0\n",
      "</s>       0           0   0          0   0.667       0    0  0              0     0        1        0\n",
      "I          0.333       0   0          0   0           0    0  0              0     0        0        0\n",
      "a          0           0   0.667      0   0           0    0  0              0     0        0        0\n",
      "different  0           0   0          0   0           0    0  0.5            0     0        0        0\n",
      "by         0           0   0          0   0.333       0    0  0              0     0        0        0\n",
      "Mulan      0           0   0          0   0           0    0  0              0     1        0        0\n",
      "<UNK>      0           0   0          0   0           0    0  0              0     0        0        0\n"
     ]
    }
   ],
   "source": [
    "table2 = [vocab]\n",
    "for i in range(0,len(vocab)):\n",
    "    temp = [vocab[i]]\n",
    "    for j in range(0,len(vocab)):\n",
    "        if(model.counts[vocab[j]]!=0):\n",
    "            temp_score = round(model.counts[[vocab[j]]][vocab[i]] / model.counts[vocab[j]],3)\n",
    "        else:\n",
    "            temp_score=0\n",
    "        temp.append(temp_score)\n",
    "    table2.append(temp)\n",
    "print(tabulate(table2, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> b. (10 pts) Calculate the sentence probability of \\<s> John read a book \\</s> using bigram \n",
    "only  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <br>According to P(w(i)|w(i-1)) = counts(w(i-1),w(i))/counts(w(i-1)),\n",
    "    <br>P(John|\\<s>) = 2/3 = 0.667\n",
    "    <br>P(read|John) = 2/2 = 1\n",
    "    <br>P(a|read) = 2/3 = 0.667\n",
    "    <br>P(book|a) = 1/2 = 0.500\n",
    "    <br>P(\\</s>|book) = 2/3 = 0.667\n",
    "    <br>0.667\\*1\\*0.667\\*0.500\\*0.667 = 0.1483704815\n",
    "    <br>Hence, the sentence probability would be 0.148.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_probablity(ngrams):\n",
    "  p = 1\n",
    "  for ngram in ngrams:\n",
    "    score = model.score(ngram[-1], ngram[:-1])\n",
    "    p *= score\n",
    "    print('    P(%s | %s): %.3f' % (ngram[-1], ' '.join(ngram[:-1]), score))\n",
    "  print('P(S):    ', p)\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['<s>', 'John', 'read', 'a', 'book', '</s>']\n",
      "    P(John | <s>): 0.667\n",
      "    P(read | John): 1.000\n",
      "    P(a | read): 0.667\n",
      "    P(book | a): 0.500\n",
      "    P(</s> | book): 0.667\n",
      "P(S):     0.14814814814814814\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentb = ['<s>', 'John', 'read', 'a', 'book','</s>']\n",
    "print('Sentence:', sentb)\n",
    "p = get_sentence_probablity(bigrams(sentb))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Above result is the answer. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> c. (10 pts) Calculate the sentence probability of \\<s> Mulan read a book \\</s> using \n",
    "bigram only </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <br>According to P(w(i)|w(i-1)) = counts(w(i-1),w(i))/counts(w(i-1)),\n",
    "    <br>P(Mulan|\\<s>) = 0/3 = 0.000\n",
    "    <br>P(read|Mulan) = 0/1 = 0.000\n",
    "    <br>P(a|read) = 2/3 = 0.667\n",
    "    <br>P(book|a) = 1/2 = 0.500\n",
    "    <br>P(\\</s>|book) = 2/3 = 0.667\n",
    "    <br>0.000\\*0.000\\*0.667\\*0.500\\*0.667 = 0\n",
    "    <br>Hence, the sentence probability would be 0.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['<s>', 'Mulan', 'read', 'a', 'book', '</s>']\n",
      "    P(Mulan | <s>): 0.000\n",
      "    P(read | Mulan): 0.000\n",
      "    P(a | read): 0.667\n",
      "    P(book | a): 0.500\n",
      "    P(</s> | book): 0.667\n",
      "P(S):     0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentc = ['<s>','Mulan','read','a','book','</s>']\n",
    "print('Sentence:', sentc)\n",
    "p = get_sentence_probablity(bigrams(sentc))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2. (30 pts) Given the training data  \n",
    "\\<s> John read her book \\</s> \n",
    "\n",
    "\\<s> I read a different book \\</s> \n",
    "\n",
    "\\<s> John read a book by Mulan \\</s> \n",
    "\n",
    "a. (10 pts) Manually list and calculate bigrams using add-1 smoothing   </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count([<s>,John]）: 3\n",
      "count([John,<s>]）: 1\n",
      "count([John,read]）: 3\n",
      "count([read,John]）: 1\n",
      "count([read,her]）: 2\n",
      "count([her,read]）: 1\n",
      "count([her,book]）: 2\n",
      "count([book,her]）: 1\n",
      "count([book,</s>]）: 3\n",
      "count([</s>,book]）: 1\n",
      "count([</s>,I]）: 1\n",
      "count([I,</s>]）: 1\n",
      "count([I,a]）: 1\n",
      "count([a,I]）: 1\n",
      "count([a,different]）: 2\n",
      "count([different,a]）: 1\n",
      "count([different,by]）: 1\n",
      "count([by,different]）: 1\n",
      "count([by,Mulan]）: 2\n",
      "count([Mulan,by]）: 1\n",
      "count([Mulan,<UNK>]）: 1\n",
      "count([<UNK>,Mulan]）: 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(vocab)-1):\n",
    "    v1 = vocab[i]\n",
    "    v2 = vocab[i+1]\n",
    "    print('count(%s）: %s' % ('['+ v1 + ','+v2+']' , model.counts[[v1]][v2] + 1))\n",
    "    print('count(%s）: %s' % ('['+ v2 + ','+v1+']' , model.counts[[v2]][v1] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             <s>    John    read    her    book    </s>    I    a    different    by    Mulan    <UNK>\n",
      "---------  -----  ------  ------  -----  ------  ------  ---  ---  -----------  ----  -------  -------\n",
      "<s>            1       3       1      1       1       1    2    1            1     1        1        1\n",
      "John           1       1       3      1       1       1    1    1            1     1        1        1\n",
      "read           1       1       1      2       1       1    1    3            1     1        1        1\n",
      "her            1       1       1      1       2       1    1    1            1     1        1        1\n",
      "book           1       1       1      1       1       3    1    1            1     2        1        1\n",
      "</s>           1       1       1      1       1       1    1    1            1     1        1        1\n",
      "I              1       1       2      1       1       1    1    1            1     1        1        1\n",
      "a              1       1       1      1       2       1    1    1            2     1        1        1\n",
      "different      1       1       1      1       2       1    1    1            1     1        1        1\n",
      "by             1       1       1      1       1       1    1    1            1     1        2        1\n",
      "Mulan          1       1       1      1       1       2    1    1            1     1        1        1\n",
      "<UNK>          1       1       1      1       1       1    1    1            1     1        1        1\n"
     ]
    }
   ],
   "source": [
    "table3 = [vocab]\n",
    "for i in range(0,len(vocab)):\n",
    "    temp = [vocab[i]]\n",
    "    for j in range(0,len(vocab)):\n",
    "        temp.append(model.counts[[vocab[i]]][vocab[j]]+1)\n",
    "    table3.append(temp)\n",
    "print(tabulate(table3, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             <s>    John    read    her    book    </s>      I      a    different     by    Mulan    <UNK>\n",
      "---------  -----  ------  ------  -----  ------  ------  -----  -----  -----------  -----  -------  -------\n",
      "<s>        0.071   0.077   0.071  0.083   0.071   0.071  0.083  0.077        0.083  0.083    0.083    0.091\n",
      "John       0.214   0.077   0.071  0.083   0.071   0.071  0.083  0.077        0.083  0.083    0.083    0.091\n",
      "read       0.071   0.231   0.071  0.083   0.071   0.071  0.167  0.077        0.083  0.083    0.083    0.091\n",
      "her        0.071   0.077   0.143  0.083   0.071   0.071  0.083  0.077        0.083  0.083    0.083    0.091\n",
      "book       0.071   0.077   0.071  0.167   0.071   0.071  0.083  0.154        0.167  0.083    0.083    0.091\n",
      "</s>       0.071   0.077   0.071  0.083   0.214   0.071  0.083  0.077        0.083  0.083    0.167    0.091\n",
      "I          0.143   0.077   0.071  0.083   0.071   0.071  0.083  0.077        0.083  0.083    0.083    0.091\n",
      "a          0.071   0.077   0.214  0.083   0.071   0.071  0.083  0.077        0.083  0.083    0.083    0.091\n",
      "different  0.071   0.077   0.071  0.083   0.071   0.071  0.083  0.154        0.083  0.083    0.083    0.091\n",
      "by         0.071   0.077   0.071  0.083   0.143   0.071  0.083  0.077        0.083  0.083    0.083    0.091\n",
      "Mulan      0.071   0.077   0.071  0.083   0.071   0.071  0.083  0.077        0.083  0.167    0.083    0.091\n",
      "<UNK>      0.071   0.077   0.071  0.083   0.071   0.071  0.083  0.077        0.083  0.083    0.083    0.091\n"
     ]
    }
   ],
   "source": [
    "table4 = [vocab]\n",
    "for i in range(0,len(vocab)):\n",
    "    temp = [vocab[i]]\n",
    "    for j in range(0,len(vocab)):\n",
    "        temp_score = round((model.counts[[vocab[j]]][vocab[i]]+1) / (model.counts[vocab[j]]+11),3)\n",
    "        temp.append(temp_score)\n",
    "    table4.append(temp)\n",
    "print(tabulate(table4, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> b. (10 pts) Calculate the sentence probability of \\<s> John read a book \\</s> using bigram \n",
    "only   </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <br>According to P(w(i)|w(i-1)) = (counts(w(i-1),w(i))+1)/(counts(w(i-1))+v),(v=11, vocab size)\n",
    "    <br>P(John|\\<s>) = 3/14 = 0.214\n",
    "    <br>P(read|John) = 3/13 = 0.231\n",
    "    <br>P(a|read) = 3/14 = 0.214\n",
    "    <br>P(book|a) = 2/13 = 0.154\n",
    "    <br>P(\\</s>|book) = 3/14 = 0.214\n",
    "    <br>0.214\\*0.231\\*0.214\\*0.154\\*0.214 = 0.00034863743\n",
    "    <br>Hence, the sentence probability would be 0.00034863743.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "John\n",
      "read\n",
      "her\n",
      "book\n",
      "</s>\n",
      "I\n",
      "a\n",
      "different\n",
      "by\n",
      "Mulan\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "for v in model.vocab:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_add1_prob(unigrams_count, bigrams_count):\n",
    "    return round((bigrams_count + 1)/(unigrams_count + 11),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_probablity_add1(ngrams):\n",
    "  p = 1\n",
    "  for ngram in ngrams:\n",
    "    v1 = ngram[0]\n",
    "    v2 = ngram[1]\n",
    "    bigrams_count = model.counts[[v1]][v2]\n",
    "    unigrams_count = model.counts[v1]\n",
    "    score = get_add1_prob(unigrams_count,bigrams_count)\n",
    "    p *= score\n",
    "    print('    P(%s | %s): %.3f' % (ngram[-1], ' '.join(ngram[:-1]), score))\n",
    "  print('P(S):    ', p)\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['<s>', 'John', 'read', 'a', 'book', '</s>']\n",
      "    P(John | <s>): 0.214\n",
      "    P(read | John): 0.231\n",
      "    P(a | read): 0.214\n",
      "    P(book | a): 0.154\n",
      "    P(</s> | book): 0.214\n",
      "P(S):     0.000348637437456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:', sentb)\n",
    "p = get_sentence_probablity_add1(bigrams(sentb))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> c. (10 pts) Calculate the sentence probability of \\<s> Mulan read a book \\</s> using bigram only </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <br>According to P(w(i)|w(i-1)) = counts((w(i-1),w(i))+1)/counts((w(i-1))+v),(v = 11, vocab size)\n",
    "    <br>P(Mulan|\\<s>) = 1/14 = 0.071\n",
    "    <br>P(read|Mulan) = 1/12 = 0.083\n",
    "    <br>P(a|read) = 3/14 = 0.214\n",
    "    <br>P(book|a) = 2/13 = 0.154\n",
    "    <br>P(\\</s>|book) = 3/14 = 0.214\n",
    "    <br>0.071\\*0.083\\*0.214\\*0.154\\*0.214 = 0.00004156087\n",
    "    <br>Hence, the sentence probability would be 0.0000415.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['<s>', 'Mulan', 'read', 'a', 'book', '</s>']\n",
      "    P(Mulan | <s>): 0.071\n",
      "    P(read | Mulan): 0.083\n",
      "    P(a | read): 0.214\n",
      "    P(book | a): 0.154\n",
      "    P(</s> | book): 0.214\n",
      "P(S):     4.1560877511999996e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Sentence:', sentc)\n",
    "p = get_sentence_probablity_add1(bigrams(sentc))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3. (10 pts) Suppose you are doing bag-of-words text classification on a document. The raw input is a \n",
    "single string containing the text of the entire document. Describe the pipeline to go from the raw \n",
    "input to a feature vector. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "First, vocabulary should be made. Vocabulary would be a set of unique words from the input string.\n",
    "<br>Then, a feature vector representing the frequencies of each unique words should be made. Each value of the feature vector would represent the frequency of each unique word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4. (30 pts) The chemprot_sample_abstracts.tsv file contains plain-text, UTF8-encoded CHEMPROT \n",
    "sample set PubMed record in a tab-separated format with the following three columns: \n",
    "a) Article identifier (PMID, PubMed identifier)  \n",
    "b) Title of the article \n",
    "c) Abstract of the article \n",
    "In total 50 records are provided in this sample set, where each line contains a single PMID, title, and \n",
    "abstract separated by tabulators. \n",
    "a. (10 pts) Calculate the number of sentences, the number of words, and the number of lemmas in \n",
    "the file  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would be using Scispacy which deals with biomedical, scientific, and clinical text.\n",
    "It is known that it has better performance than nltk and when I used Spanza, it had some errors of tagging with some formats. (Reference: https://github.com/stanfordnlp/stanza/issues/862)\n",
    "Therefore, to deal with PubMed natural text data, I chose Scispacy tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run my code, first, download spacy to get en models and load en_core_seb_sm. (Reference: https://allenai.github.io/scispacy/) I chose en_core_seb_sm model because the vocabulary wasn't used more than 50k. Then, by spliting the \"chemprot_sample_abstracts.tsv\" file with tab (/t). To use the file, save the file in the same directory with this jupyter notebook file. I only chose second and third columns to add to the text variable because the first column is PMID. With this text, I made a document to traing the text. With for loops and using set collection, which doesn't contain duplicate items, to count the numbers of unique sentences, words, and lemmas. To visualize the results in table, I used tabulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the source codes. Comments are with the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl#egg=en_core_web_sm==3.0.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (3.0.0)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from en-core-web-sm==3.0.0) (3.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.13)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: jinja2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.9.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.54.1)\n",
      "Requirement already satisfied: setuptools in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (50.3.0.post20201006)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6; python_version < \"3.7\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8)\n",
      "Requirement already satisfied: contextvars<3,>=2.4; python_version < \"3.7\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.3.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.25.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: immutables>=0.9 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from contextvars<3,>=2.4; python_version < \"3.7\"->thinc<8.1.0,>=8.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.16)\n",
      "Installing collected packages: certifi\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2016.9.26\n",
      "\u001b[31mERROR: Cannot uninstall 'certifi'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en ## downloading en models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy ##importing spacy and load en_core_seb_sm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scispacy in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: nmslib>=1.7.3.6 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scispacy) (2.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scispacy) (2.24.0)\n",
      "Requirement already satisfied: conllu in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scispacy) (4.4.1)\n",
      "Requirement already satisfied: pysbd in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scispacy) (0.3.4)\n",
      "Requirement already satisfied: numpy in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scispacy) (1.19.2)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scispacy) (3.0.7)\n",
      "Requirement already satisfied: joblib in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scispacy) (0.17.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scispacy) (0.23.2)\n",
      "Requirement already satisfied: psutil in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.0)\n",
      "Requirement already satisfied: pybind11<2.6.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.10)\n",
      "Requirement already satisfied: setuptools in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (50.3.0.post20201006)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (8.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (20.4)\n",
      "Requirement already satisfied: jinja2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (2.11.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (4.54.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.7.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scikit-learn>=0.20.3->scispacy) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from scikit-learn>=0.20.3->scispacy) (1.5.2)\n",
      "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.0->scispacy) (0.8)\n",
      "Requirement already satisfied: contextvars<3,>=2.4; python_version < \"3.7\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.3->spacy<3.1.0,>=3.0.0->scispacy) (2.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->scispacy) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->scispacy) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->scispacy) (1.1.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->scispacy) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.1.0,>=3.0.0->scispacy) (3.3.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->scispacy) (5.2.1)\n",
      "Requirement already satisfied: immutables>=0.9 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from contextvars<3,>=2.4; python_version < \"3.7\"->thinc<8.1.0,>=8.0.3->spacy<3.1.0,>=3.0.0->scispacy) (0.16)\n",
      "Installing collected packages: certifi\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2016.9.26\n",
      "\u001b[31mERROR: Cannot uninstall 'certifi'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz\n",
      "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz (33.1 MB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): en-core-sci-sm==0.3.0 from https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages\n",
      "Requirement already satisfied: spacy>=2.3.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from en-core-sci-sm==0.3.0) (3.0.7)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (0.3.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jinja2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (2.11.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (2.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (20.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (1.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (0.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (0.6.1)\n",
      "Requirement already satisfied: setuptools in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (50.3.0.post20201006)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (2.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (1.19.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (8.0.13)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (4.54.1)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (3.7.4.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from spacy>=2.3.1->en-core-sci-sm==0.3.0) (1.8.2)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from typer<0.4.0,>=0.3.0->spacy>=2.3.1->en-core-sci-sm==0.3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from jinja2->spacy>=2.3.1->en-core-sci-sm==0.3.0) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from catalogue<2.1.0,>=2.0.4->spacy>=2.3.1->en-core-sci-sm==0.3.0) (3.3.0)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.1->en-core-sci-sm==0.3.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.1->en-core-sci-sm==0.3.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.1->en-core-sci-sm==0.3.0) (1.25.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from packaging>=20.0->spacy>=2.3.1->en-core-sci-sm==0.3.0) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from packaging>=20.0->spacy>=2.3.1->en-core-sci-sm==0.3.0) (1.15.0)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from pathy>=0.3.5->spacy>=2.3.1->en-core-sci-sm==0.3.0) (5.2.1)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6; python_version < \"3.7\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from pathy>=0.3.5->spacy>=2.3.1->en-core-sci-sm==0.3.0) (0.8)\n",
      "Requirement already satisfied: contextvars<3,>=2.4; python_version < \"3.7\" in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from thinc<8.1.0,>=8.0.3->spacy>=2.3.1->en-core-sci-sm==0.3.0) (2.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /Users/dykim/opt/anaconda3/envs/bioinformatics_lab/lib/python3.6/site-packages (from contextvars<3,>=2.4; python_version < \"3.7\"->thinc<8.1.0,>=8.0.3->spacy>=2.3.1->en-core-sci-sm==0.3.0) (0.16)\n",
      "Building wheels for collected packages: en-core-sci-sm\n",
      "  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.3.0-py3-none-any.whl size=33119278 sha256=26576fee059166da2f8d2a7f695d1d4b12c6f3561ddcd31365f7f363229908cd\n",
      "  Stored in directory: /Users/dykim/Library/Caches/pip/wheels/c5/18/de/3cfa928406177be784b0e06bb1458d03bd6b746cb921f7adaf\n",
      "Successfully built en-core-sci-sm\n",
      "Installing collected packages: certifi\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2016.9.26\n",
      "\u001b[31mERROR: Cannot uninstall 'certifi'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scispacy ##install scispacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm ##load en_core_web_sm model\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re ##make text string by opening \"chemprot_sample_abstracts.tsv\" file\n",
    "f = open(\"chemprot_sample_abstracts.tsv\")\n",
    "lst= [ x for x in f.readlines()]\n",
    "text = ''\n",
    "for i in lst:\n",
    "    temp = i.split(\"\\t\")                    ##split by tab\n",
    "    text = text + temp[1]+' '+temp[2]       ##only choose title and abstract, not PMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)           ##make document by training with en_core_web_sm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n"
     ]
    }
   ],
   "source": [
    "sents_set = set()                            ##use set collection to get unique sentences\n",
    "for sentence in enumerate(doc.sents):\n",
    "    sents_set.add(sentence)\n",
    "print(len(sents_set))                        ## get the number of unique sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3182"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_set = set()                            ##use set collection to get unique words\n",
    "for i, sentence in enumerate(doc.sents):\n",
    "    for i, word in enumerate(sentence):\n",
    "        words_set.add(word.text)\n",
    "len(words_set)                               ##get the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2737"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lems_set = set()                             ##use set collection to get unique lemmas\n",
    "for i, sentence in enumerate(doc.sents):\n",
    "    for i, word in enumerate(sentence):\n",
    "        lems_set.add(word.lemma_)\n",
    "len(lems_set)                                ##get the number of unique lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types       num\n",
      "--------  -----\n",
      "sentence    594\n",
      "word       3182\n",
      "lemma      2737\n"
     ]
    }
   ],
   "source": [
    "table_text = [['types','num']]                     ##use tabulate to make a table for the results\n",
    "sen_row = ['sentence', len(sents_set)]\n",
    "word_row = ['word', len(words_set)]\n",
    "lem_row = ['lemma',len(lems_set)]\n",
    "table_text.append(sen_row)\n",
    "table_text.append(word_row)\n",
    "table_text.append(lem_row)\n",
    "print(tabulate(table_text, headers='firstrow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
